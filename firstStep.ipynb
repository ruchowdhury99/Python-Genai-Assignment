{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b6d99a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unstructured\n",
      "  Downloading unstructured-0.17.2-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting langchain\n",
      "  Using cached langchain-0.3.23-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-community\n",
      "  Using cached langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting langchain-huggingface\n",
      "  Using cached langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.9.10-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting pgvector\n",
      "  Downloading pgvector-0.4.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting chardet (from unstructured)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting filetype (from unstructured)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting python-magic (from unstructured)\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting lxml (from unstructured)\n",
      "  Downloading lxml-5.3.2-cp311-cp311-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting nltk (from unstructured)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting requests (from unstructured)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting beautifulsoup4 (from unstructured)\n",
      "  Using cached beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting emoji (from unstructured)\n",
      "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting dataclasses-json (from unstructured)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting python-iso639 (from unstructured)\n",
      "  Downloading python_iso639-2025.2.18-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting langdetect (from unstructured)\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ------------------------------ ------- 786.4/981.5 kB 6.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 981.5/981.5 kB 3.5 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting numpy (from unstructured)\n",
      "  Using cached numpy-2.2.4-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting rapidfuzz (from unstructured)\n",
      "  Downloading rapidfuzz-3.13.0-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting backoff (from unstructured)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from unstructured) (4.13.2)\n",
      "Collecting unstructured-client (from unstructured)\n",
      "  Downloading unstructured_client-0.32.3-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting wrapt (from unstructured)\n",
      "  Downloading wrapt-1.17.2-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting tqdm (from unstructured)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from unstructured) (7.0.0)\n",
      "Collecting python-oxmsg (from unstructured)\n",
      "  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting html5lib (from unstructured)\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.51 (from langchain)\n",
      "  Using cached langchain_core-0.3.52-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain)\n",
      "  Using cached langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith<0.4,>=0.1.17 (from langchain)\n",
      "  Using cached langsmith-0.3.31-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
      "  Using cached pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Using cached sqlalchemy-2.0.40-cp311-cp311-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain)\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
      "  Using cached aiohttp-3.11.16-cp311-cp311-win_amd64.whl.metadata (8.0 kB)\n",
      "Collecting tenacity!=8.4.0,<10,>=8.1.0 (from langchain-community)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Using cached pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting huggingface-hub>=0.23.0 (from langchain-huggingface)\n",
      "  Using cached huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting sentence-transformers>=2.6.0 (from langchain-huggingface)\n",
      "  Using cached sentence_transformers-4.1.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tokenizers>=0.19.1 (from langchain-huggingface)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting transformers>=4.39.0 (from langchain-huggingface)\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached frozenlist-1.5.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached multidict-6.4.3-cp311-cp311-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Using cached propcache-0.3.1-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading yarl-1.20.0-cp311-cp311-win_amd64.whl.metadata (74 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->unstructured)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->unstructured)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.23.0->langchain-huggingface)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.23.0->langchain-huggingface)\n",
      "  Using cached fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (24.2)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.51->langchain)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached orjson-3.10.16-cp311-cp311-win_amd64.whl.metadata (42 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached zstandard-0.23.0-cp311-cp311-win_amd64.whl.metadata (3.0 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.1 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached pydantic_core-2.33.1-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
      "  Using cached typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->unstructured)\n",
      "  Using cached charset_normalizer-3.4.1-cp311-cp311-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->unstructured)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->unstructured)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->unstructured)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Using cached torch-2.6.0-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting scikit-learn (from sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Using cached scikit_learn-1.6.1-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting scipy (from sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Using cached scipy-1.15.2-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting Pillow (from sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Using cached pillow-11.2.1-cp311-cp311-win_amd64.whl.metadata (9.1 kB)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Using cached greenlet-3.2.0-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from tqdm->unstructured) (0.4.6)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.39.0->langchain-huggingface)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers>=4.39.0->langchain-huggingface)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->unstructured)\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from html5lib->unstructured) (1.17.0)\n",
      "Collecting webencodings (from html5lib->unstructured)\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting click (from nltk->unstructured)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting joblib (from nltk->unstructured)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting olefile (from python-oxmsg->unstructured)\n",
      "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting aiofiles>=24.1.0 (from unstructured-client->unstructured)\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting cryptography>=3.1 (from unstructured-client->unstructured)\n",
      "  Downloading cryptography-44.0.2-cp39-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting eval-type-backport>=0.2.0 (from unstructured-client->unstructured)\n",
      "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from unstructured-client->unstructured) (1.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from unstructured-client->unstructured) (2.9.0.post0)\n",
      "Collecting cffi>=1.12 (from cryptography>=3.1->unstructured-client->unstructured)\n",
      "  Downloading cffi-1.17.1-cp311-cp311-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached httpcore-1.0.8-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
      "  Using cached MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Downloading unstructured-0.17.2-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 0.8/1.8 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 4.2 MB/s eta 0:00:00\n",
      "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
      "Using cached langchain-0.3.23-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
      "Using cached langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
      "Downloading psycopg2_binary-2.9.10-cp311-cp311-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 0.8/1.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 3.6 MB/s eta 0:00:00\n",
      "Downloading pgvector-0.4.0-py3-none-any.whl (27 kB)\n",
      "Using cached aiohttp-3.11.16-cp311-cp311-win_amd64.whl (442 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Using cached langchain_core-0.3.52-py3-none-any.whl (433 kB)\n",
      "Using cached langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Using cached langsmith-0.3.31-py3-none-any.whl (358 kB)\n",
      "Using cached numpy-2.2.4-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "Using cached pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
      "Using cached pydantic_core-2.33.1-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "Using cached pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
      "Using cached PyYAML-6.0.2-cp311-cp311-win_amd64.whl (161 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached sentence_transformers-4.1.0-py3-none-any.whl (345 kB)\n",
      "Using cached sqlalchemy-2.0.40-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
      "   ---------------------------------------- 0.0/590.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 590.6/590.6 kB 5.3 MB/s eta 0:00:00\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Downloading lxml-5.3.2-cp311-cp311-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 1.0/3.8 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.8/3.8 MB 4.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 2.6/3.8 MB 4.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.4/3.8 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 3.9 MB/s eta 0:00:00\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.8/1.5 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 3.6 MB/s eta 0:00:00\n",
      "Downloading python_iso639-2025.2.18-py3-none-any.whl (167 kB)\n",
      "Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n",
      "Downloading rapidfuzz-3.13.0-cp311-cp311-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 1.0/1.6 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 4.2 MB/s eta 0:00:00\n",
      "Downloading unstructured_client-0.32.3-py3-none-any.whl (180 kB)\n",
      "Downloading wrapt-1.17.2-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp311-cp311-win_amd64.whl (102 kB)\n",
      "Downloading cryptography-44.0.2-cp39-abi3-win_amd64.whl (3.2 MB)\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 1.0/3.2 MB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.8/3.2 MB 4.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.6/3.2 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.2/3.2 MB 4.0 MB/s eta 0:00:00\n",
      "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Using cached frozenlist-1.5.0-cp311-cp311-win_amd64.whl (51 kB)\n",
      "Using cached fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Using cached greenlet-3.2.0-cp311-cp311-win_amd64.whl (295 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.8-py3-none-any.whl (78 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached multidict-6.4.3-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Using cached orjson-3.10.16-cp311-cp311-win_amd64.whl (133 kB)\n",
      "Using cached propcache-0.3.1-cp311-cp311-win_amd64.whl (45 kB)\n",
      "Using cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Using cached torch-2.6.0-cp311-cp311-win_amd64.whl (204.2 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Downloading yarl-1.20.0-cp311-cp311-win_amd64.whl (93 kB)\n",
      "Using cached zstandard-0.23.0-cp311-cp311-win_amd64.whl (495 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
      "Using cached pillow-11.2.1-cp311-cp311-win_amd64.whl (2.7 MB)\n",
      "Using cached scikit_learn-1.6.1-cp311-cp311-win_amd64.whl (11.1 MB)\n",
      "Using cached scipy-1.15.2-cp311-cp311-win_amd64.whl (41.2 MB)\n",
      "Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Downloading cffi-1.17.1-cp311-cp311-win_amd64.whl (181 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (pyproject.toml): started\n",
      "  Building wheel for langdetect (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993363 sha256=d34a856d9b4712f0262cff1cb9b03c721a2d01da20e0ff8bd5f22179e41429ec\n",
      "  Stored in directory: c:\\users\\ruchowdhury\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local\\pip\\cache\\wheels\\0a\\f2\\b2\\e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "Successfully built langdetect\n",
      "Installing collected packages: webencodings, mpmath, filetype, zstandard, wrapt, urllib3, typing-inspection, tqdm, threadpoolctl, tenacity, sympy, soupsieve, sniffio, safetensors, regex, rapidfuzz, PyYAML, python-magic, python-iso639, python-dotenv, pypdf, pydantic-core, pycparser, psycopg2-binary, propcache, Pillow, orjson, olefile, numpy, networkx, mypy-extensions, multidict, marshmallow, MarkupSafe, lxml, langdetect, jsonpointer, joblib, idna, httpx-sse, html5lib, h11, greenlet, fsspec, frozenlist, filelock, eval-type-backport, emoji, click, charset-normalizer, chardet, certifi, backoff, attrs, annotated-types, aiohappyeyeballs, aiofiles, yarl, typing-inspect, SQLAlchemy, scipy, requests, python-oxmsg, pydantic, pgvector, nltk, jsonpatch, jinja2, httpcore, cffi, beautifulsoup4, anyio, aiosignal, torch, scikit-learn, requests-toolbelt, pydantic-settings, huggingface-hub, httpx, dataclasses-json, cryptography, aiohttp, unstructured-client, tokenizers, langsmith, unstructured, transformers, langchain-core, sentence-transformers, langchain-text-splitters, langchain-huggingface, langchain, langchain-community\n",
      "Successfully installed MarkupSafe-3.0.2 Pillow-11.2.1 PyYAML-6.0.2 SQLAlchemy-2.0.40 aiofiles-24.1.0 aiohappyeyeballs-2.6.1 aiohttp-3.11.16 aiosignal-1.3.2 annotated-types-0.7.0 anyio-4.9.0 attrs-25.3.0 backoff-2.2.1 beautifulsoup4-4.13.4 certifi-2025.1.31 cffi-1.17.1 chardet-5.2.0 charset-normalizer-3.4.1 click-8.1.8 cryptography-44.0.2 dataclasses-json-0.6.7 emoji-2.14.1 eval-type-backport-0.2.2 filelock-3.18.0 filetype-1.2.0 frozenlist-1.5.0 fsspec-2025.3.2 greenlet-3.2.0 h11-0.14.0 html5lib-1.1 httpcore-1.0.8 httpx-0.28.1 httpx-sse-0.4.0 huggingface-hub-0.30.2 idna-3.10 jinja2-3.1.6 joblib-1.4.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.23 langchain-community-0.3.21 langchain-core-0.3.52 langchain-huggingface-0.1.2 langchain-text-splitters-0.3.8 langdetect-1.0.9 langsmith-0.3.31 lxml-5.3.2 marshmallow-3.26.1 mpmath-1.3.0 multidict-6.4.3 mypy-extensions-1.0.0 networkx-3.4.2 nltk-3.9.1 numpy-2.2.4 olefile-0.47 orjson-3.10.16 pgvector-0.4.0 propcache-0.3.1 psycopg2-binary-2.9.10 pycparser-2.22 pydantic-2.11.3 pydantic-core-2.33.1 pydantic-settings-2.8.1 pypdf-5.4.0 python-dotenv-1.1.0 python-iso639-2025.2.18 python-magic-0.4.27 python-oxmsg-0.0.2 rapidfuzz-3.13.0 regex-2024.11.6 requests-2.32.3 requests-toolbelt-1.0.0 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.15.2 sentence-transformers-4.1.0 sniffio-1.3.1 soupsieve-2.6 sympy-1.13.1 tenacity-9.1.2 threadpoolctl-3.6.0 tokenizers-0.21.1 torch-2.6.0 tqdm-4.67.1 transformers-4.51.3 typing-inspect-0.9.0 typing-inspection-0.4.0 unstructured-0.17.2 unstructured-client-0.32.3 urllib3-2.4.0 webencodings-0.5.1 wrapt-1.17.2 yarl-1.20.0 zstandard-0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U unstructured pypdf langchain langchain-community langchain-huggingface psycopg2-binary pgvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05293104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from python-docx) (5.3.2)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from python-docx) (4.13.2)\n",
      "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "Installing collected packages: python-docx\n",
      "Successfully installed python-docx-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "479a3904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import things from langchain\n",
    "# import os for managing file paths\n",
    "import os\n",
    "\n",
    "# import document_loaders for loading text and PDFs\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader, UnstructuredWordDocumentLoader\n",
    "\n",
    "#import text_splitter for splitting large texts into smaller chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# import embeddings for converting text into numerical vectors\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "#import vectorstore for storing and retrieving embeddings\n",
    "from langchain.vectorstores.pgvector import PGVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6b29d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document loaded successfully: ./data/Python SRS.docx\n",
      "Document split into 14 chunks\n",
      "PgVector store created successfully <langchain_community.vectorstores.pgvector.PGVector object at 0x0000019551223B50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruchowdhury\\Desktop\\Final Assignment\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\pgvector.py:488: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata. Please note that filtering operators have been changed when using JSONB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create a db migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  store = cls(\n"
     ]
    }
   ],
   "source": [
    "# Loading docuement function\n",
    "def load_document(file_path):\n",
    "    \"\"\"\n",
    "        load a document from a file path of extension txt, pdf, or docx.\n",
    "        Args: file_path (str): file path of the document\n",
    "        Returns: langchain.document.Document: a list of document object\n",
    "    \"\"\"\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    if ext == '.txt':\n",
    "        loader = TextLoader(file_path, encoding='utf-8')\n",
    "    elif ext == '.pdf':\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    elif ext == '.docx':\n",
    "        loader = UnstructuredWordDocumentLoader(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {ext}\")\n",
    "    return loader.load()\n",
    "\n",
    "# Split docuement function\n",
    "def split_document(document, chunk_size=500, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "        Split a document into smaller chunks with a specified overlap.\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return text_splitter.split_documents(document)\n",
    "\n",
    "# Set up PGVector store from document chunks using HuggingFace embeddings\n",
    "def create_pgvector_store(chunks, connection_string, collection_name=\"srs_documents\"):\n",
    "    \"\"\"\n",
    "        Creates a PgVector store from a list of document chunks using HuggingFace embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "    vector_store = PGVector.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        collection_name=collection_name,\n",
    "        connection_string=connection_string,\n",
    "    )\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to your document\n",
    "    document_path = \"./data/Python SRS.docx\"\n",
    "\n",
    "    try:\n",
    "        # Step 1. Load the document using file path\n",
    "        document = load_document(document_path)\n",
    "        print(f\"Document loaded successfully: {document_path}\")\n",
    "\n",
    "        # Step 2. Split the document into smaller chunks\n",
    "        chunks = split_document(document)\n",
    "        print(f\"Document split into {len(chunks)} chunks\")\n",
    "\n",
    "        # Step 3. Set up PgVector for storing and retrieving embeddings\n",
    "        connection_string = \"postgresql+psycopg2://Rupantar:1234@localhost/genairagdb\"\n",
    "        vector_store = create_pgvector_store(chunks, connection_string)\n",
    "        print(\"PgVector store created successfully\", vector_store)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occured : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e949f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5edf535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b34f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruchowdhury\\Desktop\\Final Assignment\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\pgvector.py:488: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata. Please note that filtering operators have been changed when using JSONB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create a db migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  store = cls(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Document 1 ---\n",
      "Functional requirement 1: The system shall...\n",
      "\n",
      "--- Document 2 ---\n",
      "Functional requirement 2: The user shall be able to...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Retrieving the embeddings from the pgvector\n",
    "\n",
    "# from langchain.vectorstores.pgvector import PGVector\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain.schema import Document\n",
    "\n",
    "# # Example SRS documents (replace with your actual data)\n",
    "# srs_documents = [\n",
    "#     \"Functional requirement 1: The system shall...\",\n",
    "#     \"Functional requirement 2: The user shall be able to...\",\n",
    "#     # Add more SRS document chunks as needed\n",
    "# ]\n",
    "\n",
    "# # Initialize the 768-dim embedding model\n",
    "# embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# # Define your new collection name (to separate from previous one)\n",
    "# collection_name = \"srs_chunks_768\"\n",
    "\n",
    "# # Initialize PGVector vectorstore\n",
    "# vectorstore = PGVector.from_documents(\n",
    "#     documents=[Document(page_content=chunk) for chunk in srs_documents],\n",
    "#     embedding=embedding_model,\n",
    "#     collection_name=collection_name,\n",
    "#     connection_string=\"postgresql+psycopg2://Rupantar:1234@localhost:5432/genairagdb\"\n",
    "# )\n",
    "\n",
    "# # Create a retriever to fetch relevant SRS chunks\n",
    "# retriever = vectorstore.as_retriever()\n",
    "\n",
    "# # Retrieve documents based on a query\n",
    "# docs = retriever.get_relevant_documents('Extract all functional requirements')\n",
    "\n",
    "# # Print preview of each document (first 500 characters)\n",
    "# for i, doc in enumerate(docs):\n",
    "#     print(f\"--- Document {i+1} ---\")\n",
    "#     print(doc.page_content[:500])\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b419c0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (0.22.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from groq) (2.11.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from groq) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84823cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (0.3.23)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: langchain-groq in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (0.3.2)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from langchain) (0.3.52)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from langchain) (0.3.31)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from langchain) (2.11.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from langchain-groq) (0.22.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (0.28.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (4.13.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ruchowdhury\\desktop\\final assignment\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain python-dotenv langchain-groq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b79ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from groq import Groq\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "\n",
    "# # Load environment variables from .env file\n",
    "# load_dotenv()\n",
    "\n",
    "# # Access the API key\n",
    "# api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "\n",
    "# # Initialize the client with your API key\n",
    "# llm = Groq(api_key=api_key)\n",
    "\n",
    "# # Create a function to invoke LLaMA-3\n",
    "# def query_llama3(prompt):\n",
    "#     response = llm.chat.completions.create(\n",
    "#         model=\"llama3-8b-8192\",  # or \"llama3-70b-8192\" if you prefer\n",
    "#         messages=[\n",
    "#             {\"role\": \"user\", \"content\": prompt}\n",
    "#         ]\n",
    "#     )\n",
    "#     return response.choices[0].message.content\n",
    "\n",
    "# def extract_functional_requirements(chunks):\n",
    "#     prompt_template = '''Extract all functional requirements from the following software specification chunk:\n",
    "\n",
    "# {srs_chunk}\n",
    "\n",
    "# Return the result in JSON format with the following fields:\n",
    "# - id\n",
    "# - description\n",
    "# - module (if any)'''\n",
    "\n",
    "#     results = []\n",
    "#     for chunk in chunks:\n",
    "#         response = query_llama3(prompt_template.format(srs_chunk=chunk))\n",
    "#         results.append(response)\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88d50fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I was unable to extract the required points as the context seems to be incomplete and repetitive. However, I can provide a possible interpretation of the given information.\n",
      "\n",
      "Assuming the context is referring to the Leave Management System (LMS) and Pods, here's a possible extraction of the requirements in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"endpoints\": [\n",
      "    {\n",
      "      \"path\": \"/expense\",\n",
      "      \"method\": \"POST\",\n",
      "      \"params\": [\"employee_id\", \"expense_amount\", \"supporting_documents\"],\n",
      "      \"description\": \"Submit expense with valid supporting documents\"\n",
      "    },\n",
      "    {\n",
      "      \"path\": \"/expense\",\n",
      "      \"method\": \"GET\",\n",
      "      \"params\": [\"employee_id\"],\n",
      "      \"description\": \"Retrieve expense history for a specific employee\"\n",
      "    }\n",
      "  ],\n",
      "  \"logic\": \"The system should allow employees to submit expenses after completing training modules. The system should validate the submitted expenses and ensure they have valid supporting documents. The system should also allow managers to view and approve/deny expenses.\",\n",
      "  \"schema\": {\n",
      "    \"tables\": [\n",
      "      {\n",
      "        \"name\": \"employees\",\n",
      "        \"columns\": [\"employee_id\", \"name\", \"email\"]\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"expenses\",\n",
      "        \"columns\": [\"expense_id\", \"employee_id\", \"expense_amount\", \"status\"]\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"supporting_documents\",\n",
      "        \"columns\": [\"document_id\", \"expense_id\", \"document_type\"]\n",
      "      }\n",
      "    ],\n",
      "    \"relationships\": [\n",
      "      {\n",
      "        \"table1\": \"employees\",\n",
      "        \"table2\": \"expenses\",\n",
      "        \"relationship_type\": \"one-to-many\"\n",
      "      },\n",
      "      {\n",
      "        \"table1\": \"expenses\",\n",
      "        \"table2\": \"supporting_documents\",\n",
      "        \"relationship_type\": \"one-to-many\"\n",
      "      }\n",
      "    ],\n",
      "    \"constraints\": [\n",
      "      {\n",
      "        \"table\": \"expenses\",\n",
      "        \"column\": \"status\",\n",
      "        \"constraint_type\": \"enum\",\n",
      "        \"values\": [\"pending\", \"approved\", \"denied\"]\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"auth\": \"The system should use a combination of username/password and role-based access control to authenticate and authorize users. Employees should have read-only access to their own expense history, while managers should have read-write access to all expenses.\"\n",
      "}\n",
      "```\n",
      "\n",
      "Please note that this is a possible interpretation and may not accurately reflect the actual requirements. The provided context is incomplete and repetitive, making it difficult to extract accurate requirements.\n"
     ]
    }
   ],
   "source": [
    "# Code Generated by Sidekick is for learning and experimentation purposes only. \n",
    "\n",
    "# 1. Imports\n",
    "# from langchain.retrievers import VectorStoreRetriever\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# 2. Wrap the vector store retriever in a retriever object\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# 3. Define the prompt template for our question-answering chain\n",
    "template = \"\"\"\n",
    "You are an assistant that extracts the functional requirements from an SRS document of a project.\n",
    "Your task is to extract the required points from the given context.\n",
    "Given the context (pulled from the SRS):\n",
    "\n",
    "{context}\n",
    "\n",
    "Please extract and return **only** the following requirements in JSON format with these keys:\n",
    "- \"endpoints\": list of objects {{ \"path\": \"\", \"method\": \"\", \"params\":[...], \"description\": \"\" }}\n",
    "- \"logic\": description of the system's business rules and computations\n",
    "- \"schema\": description of tables, relationships, and constraints\n",
    "- \"auth\": description of authentication and authorization mechanisms\n",
    " \"\"\"\n",
    "\n",
    "# template = \"\"\"\n",
    "#       You are an assistant that extracts backend development requirements from an SRS document.\n",
    "\n",
    "#       Given the following context from the document:\n",
    "\n",
    "#       {context}\n",
    "\n",
    "#       Extract the following in JSON format:\n",
    "\n",
    "#       {\n",
    "#         \"endpoints\": [\n",
    "#           {\n",
    "#             \"path\": \"\",\n",
    "#             \"method\": \"\",\n",
    "#             \"description\": \"\",\n",
    "#             \"params\": []\n",
    "#           }\n",
    "#         ],\n",
    "#         \"logic\": \"Summary of the core business logic and workflows.\",\n",
    "#         \"schema\": \"Describe the database tables, fields, and relationships.\",\n",
    "#         \"auth\": \"Explain the login, JWT usage, roles (user, manager), and access control.\"\n",
    "#       }\n",
    "\n",
    "#       Only return JSON. Do not add any explanation.\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"context\"], template=template)\n",
    "\n",
    "# 4. Load the Groq LLM (Hypothetical valid model name)\n",
    "groqLLM = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",  # Replace with a valid model name\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "# 5. Create the retrieval chain with the retriever and the prompt\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=groqLLM,  # Use Groq LLM\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "# 6. Run the chain on the document\n",
    "query = \"Extract the functional requirements from the document for upcoming project.\"\n",
    "response = qa_chain.run(query)\n",
    "\n",
    "# Print the output\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
